{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "# np.random.seed(1237)\n",
    " \n",
    "# Source file directory\n",
    "path_train = \"20news-19997/20_newsgroups\"\n",
    " \n",
    "files_train = skds.load_files(path_train,load_content=False)\n",
    " \n",
    "label_index = files_train.target\n",
    "label_names = files_train.target_names\n",
    "labelled_files = files_train.filenames\n",
    " \n",
    "data_tags = [\"filename\",\"category\",\"news\"]\n",
    "data_list = []\n",
    " \n",
    "# Read and add data from file to a list\n",
    "i=0\n",
    "for f in labelled_files:\n",
    "    data_list.append((f,label_names[label_index[i]],Path(f).read_text()))\n",
    "    i += 1\n",
    " \n",
    "# We have training data available as dictionary filename, category, data\n",
    "data = pd.DataFrame.from_records(data_list, columns=data_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets take 80% data as training and remaining 20% for test.\n",
    "train_size = int(len(data) * .8)\n",
    " \n",
    "train_posts = data['news'][:train_size]\n",
    "train_tags = data['category'][:train_size]\n",
    "train_files_names = data['filename'][:train_size]\n",
    " \n",
    "test_posts = data['news'][train_size:]\n",
    "test_tags = data['category'][train_size:]\n",
    "test_files_names = data['filename'][train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 news groups\n",
    "num_labels = 20\n",
    "vocab_size = 15000\n",
    "batch_size = 100\n",
    " \n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_posts)\n",
    " \n",
    "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
    " \n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# encoder = LabelBinarizer()\n",
    "# encoder.fit(train_tags)\n",
    "# y_train = encoder.transform(train_tags)\n",
    "# y_test = encoder.transform(test_tags)\n",
    "\n",
    "# encoder = LabelBinarizer()\n",
    "# encoder.fit(train_tags)\n",
    "# y_train = encoder.transform(train_tags)\n",
    "# y_test = encoder.transform(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0806 16:19:13.484101 10764 deprecation_wrapper.py:119] From C:\\Users\\coral\\Anaconda3\\envs\\CV\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0806 16:19:14.407003 10764 deprecation_wrapper.py:119] From C:\\Users\\coral\\Anaconda3\\envs\\CV\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0806 16:19:14.729438 10764 deprecation_wrapper.py:119] From C:\\Users\\coral\\Anaconda3\\envs\\CV\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0806 16:19:14.982545 10764 deprecation_wrapper.py:119] From C:\\Users\\coral\\Anaconda3\\envs\\CV\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0806 16:19:15.054524 10764 deprecation.py:506] From C:\\Users\\coral\\Anaconda3\\envs\\CV\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0806 16:19:15.358450 10764 deprecation_wrapper.py:119] From C:\\Users\\coral\\Anaconda3\\envs\\CV\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0806 16:19:15.442451 10764 deprecation_wrapper.py:119] From C:\\Users\\coral\\Anaconda3\\envs\\CV\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               7680512   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                10260     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 7,953,428\n",
      "Trainable params: 7,953,428\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0806 16:19:16.147249 10764 deprecation.py:323] From C:\\Users\\coral\\Anaconda3\\envs\\CV\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14397 samples, validate on 1600 samples\n",
      "Epoch 1/1\n",
      "14397/14397 [==============================] - 81s 6ms/step - loss: 0.6928 - acc: 0.8229 - val_loss: 0.2552 - val_acc: 0.9406\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=1,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 4s 992us/step\n",
      "Test accuracy: 0.9325000047683716\n",
      "20news-19997/20_newsgroups\\alt.atheism\\51181\n",
      "Actual label:alt.atheism\n",
      "Predicted label: alt.atheism\n",
      "20news-19997/20_newsgroups\\comp.graphics\\38985\n",
      "Actual label:comp.graphics\n",
      "Predicted label: comp.graphics\n",
      "20news-19997/20_newsgroups\\talk.politics.guns\\54710\n",
      "Actual label:talk.politics.guns\n",
      "Predicted label: talk.politics.guns\n",
      "20news-19997/20_newsgroups\\rec.sport.baseball\\104743\n",
      "Actual label:rec.sport.baseball\n",
      "Predicted label: rec.sport.baseball\n",
      "20news-19997/20_newsgroups\\talk.politics.misc\\179022\n",
      "Actual label:talk.politics.misc\n",
      "Predicted label: talk.politics.misc\n",
      "20news-19997/20_newsgroups\\sci.crypt\\15238\n",
      "Actual label:sci.crypt\n",
      "Predicted label: sci.crypt\n",
      "20news-19997/20_newsgroups\\rec.sport.hockey\\52649\n",
      "Actual label:rec.sport.hockey\n",
      "Predicted label: rec.sport.hockey\n",
      "20news-19997/20_newsgroups\\sci.space\\60223\n",
      "Actual label:sci.space\n",
      "Predicted label: sci.space\n",
      "20news-19997/20_newsgroups\\misc.forsale\\76449\n",
      "Actual label:misc.forsale\n",
      "Predicted label: misc.forsale\n",
      "20news-19997/20_newsgroups\\soc.religion.christian\\21472\n",
      "Actual label:soc.religion.christian\n",
      "Predicted label: soc.religion.christian\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "text_labels = encoder.classes_\n",
    "\n",
    "for i in range(10):\n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction[0])]\n",
    "    print(test_files_names.iloc[i])\n",
    "    print('Actual label:' + test_tags.iloc[i])\n",
    "    print(\"Predicted label: \" + predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a HDF5 file 'my_model.h5'\n",
    "model.model.save('my_model.h5')\n",
    " \n",
    "# Save Tokenizer i.e. Vocabulary\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc',\n",
       "       'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
       "       'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles',\n",
       "       'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt',\n",
       "       'sci.electronics', 'sci.med', 'sci.space',\n",
       "       'soc.religion.christian', 'talk.politics.guns',\n",
       "       'talk.politics.mideast', 'talk.politics.misc',\n",
       "       'talk.religion.misc'], dtype='<U24')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load our saved model\n",
    "from keras.models import load_model\n",
    "model = load_model('my_model.h5')\n",
    " \n",
    "# load tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "    \n",
    "    \n",
    "encoder.classes_ #LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File -> news.txt Predicted label: sci.technology\n"
     ]
    }
   ],
   "source": [
    "# These are the labels we stored from our training\n",
    "# The order is very important here.\n",
    " \n",
    "labels = np.array(['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x',\n",
    " 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball',\n",
    " 'rec.sport.football', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.technology',\n",
    " 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast',\n",
    " 'talk.politics.misc', 'talk.religion.misc'])\n",
    " \n",
    "test_files = [\"news.txt\"]\n",
    "x_data = []\n",
    "for t_f in test_files:\n",
    "    t_f_data = Path(t_f).read_text()\n",
    "    x_data.append(t_f_data)\n",
    " \n",
    "x_data_series = pd.Series(x_data)\n",
    "x_tokenized = tokenizer.texts_to_matrix(x_data_series, mode='tfidf')\n",
    " \n",
    "i=0\n",
    "for x_t in x_tokenized:\n",
    "    prediction = model.predict(np.array([x_t]))\n",
    "    predicted_label = labels[np.argmax(prediction[0])]\n",
    "    print(\"File ->\", test_files[i], \"Predicted label: \" + predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
